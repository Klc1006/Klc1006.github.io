{"meta":{"title":null,"subtitle":"","description":"","author":"Klc","url":"https://Klc1006.github.io","root":"/"},"pages":[],"posts":[{"title":"【论文调研】CONVERGING TO UNEXPLOITABLE POLICIES IN CONTINUOUS CONTROL ADVERSARIAL GAMES 笔记","slug":"AdaptiveFSP笔记","date":"2023-03-06T08:34:34.726Z","updated":"2023-03-06T09:24:57.379Z","comments":true,"path":"2023/03/06/AdaptiveFSP笔记/","link":"","permalink":"https://klc1006.github.io/2023/03/06/AdaptiveFSP%E7%AC%94%E8%AE%B0/","excerpt":"Adaptive FSP","text":"Adaptive FSP 名词解释 元策略（meta-strategies）：提出基于复杂的原博弈得到一个规模小得多的经验博弈 empirical game/meta game，通过对经验博弈的推理(meta-reasoning)得到的元策略(meta-strategies)，依托这些信息指导在原策略空间中寻找新的策略，同时使逼近可能的原博弈的均衡解。对于任意联合策略的期望收益，通过适当的方法进行估计并记录在经验收益矩阵 empirical payoff table中 混合nash均衡 image-20230128201928792 主要思想：不仅针对meta policy训练，还针对能够适应BR的adaptive deep RL agent进行训练。 Introduce CFR 主要限制是不清楚如何将其扩展到连续动作空间 FSP：简单地扩展到连续动作空间，FSP是一种求解两人零和（2p0s）博弈的迭代算法（每次迭代都可以查看） 在每次迭代中，每个玩家都会计算出对对手元策略的最佳响应 在普通FSP中，元策略是在先前迭代中计算的所有策略的统一平均值（在迭代k上，它是从迭代1到k−1的所有策略统一平均值） FSP被保证最终收敛到纳什均衡，在纳什均衡中，两个参与者的元策略都是对另一个的最优响应 vanilla FSP的局限性：学习对具有深度RL的元策略的最佳响应可能导致对元策略策略的过度拟合，从而减缓向低可利用性策略的收敛。 （ For example, in a game of soccer the kicker might adversarially figure out how to move in such a way that the population of previous goalie policies has not seen before, causing degenerate behavior (Gleave et al., 2019).） 主要贡献： Adapt FSP增强了迭代k中玩家的元策略，自适应对手主动更新其策略以应对最佳响应。自适应对手充当正则化因子regularizer ，减少对先前策略群体（population of policies）的过度拟合。 步骤 首先证明了AdaptFSP属于一类被称为广义弱化虚拟博弈（GWFP）的算法，所有这些算法都可证明在2p0s博弈中收敛到纳什均衡。 在四个表格和连续博弈中评估AdaptFSP 引入了部分可观测性：将对手的观测延迟十倍 AdaptFSP相较于生成的策略更健壮，可利用性更低，。 AdaptFSP produces policies that are more robust and less exploitable than FSP in these settings Related work fully observable 2p0s games：必须执行连续控制（agents have to perform continuous control） （Bansal et al. (2017) study fully observable 2p0s games in which the agents have to perform continuous control. ） 方法：（对手抽样） opponent sampling：当训练策略时，对手的行为从过去策略的缓冲区中采样。他们没有保留所有策略，而是使用过去策略的滑动窗口（sliding window）。 缺陷：they do not study the exploitability（可利用性） of their learned agents. the question of meta learning in two-player competitive continuous control tasks. 两人竞争性连续控制任务中的元学习问题。 （Al-Shedivat et al. (2017) study the question of meta learning in two-player competitive continuous control tasks.） 实验结果：元学习智能体表现最好Two players play several rounds against each other, and they show that the meta-learning agent performs the best out of several candidate algorithms. 特点（缺陷）：实验环境是完全可观测的，在完全可观察环境中收敛到最优策略的许多算法在部分可观察环境下不保持该特性。 本文实验会会引入部分可观测性 NFSP (Heinrich and Silver, 2016) 使用深度强化学习，从自博弈中学习到2p0s不完美信息博弈中的纳什均衡。 使用深度神经网络来近似平均策略，并使用深度神经网来学习具有RL式奖励的策略，但仅在具有小动作空间的游戏中有效。 足球比赛：球员团队可以在足球比赛中相互竞争 （Liu et al. (2019) introduce the MuJoCo Soccer domain where teams of players can compete against each other in a soccer match.） 去中心化基于种群的训练（decentralized population-based training）表现出合作和可利用性的能力 实验结果：在20分钟内收敛到纳什均衡，但是该足球环境是完全可观察的observable 最大限度地实现信息理论目标，以学习可推广到下游任务的各种技能。 （ysenbach et al. (2018) show they can maximize an information theoretic objective to learn a diverse set of skills that are generalizable to downstream tasks.） 提出了一种基于原型表示的学习不同技能的自我监督方法，该技术对下游任务具有很强的泛化能力（Yarats et al. (2021) present a self-supervised technique for learning different skills based on prototypical representations that shows strong generalization to downstream tasks.） POMDP（部分可观测马尔可夫决策过程）背景下RL的泛化 （Ghosh et al. (2021) study generalization in RL in the context of POMDPs (Partially Observable Markov Decision Process) and show classic RL algorithms do not perform as well in POMDPs as in MDPs.） 验证结果：经典RL算法在POMDP中的表现不如MDP，提出了一种基于集成的方法，并表明它比普通RL更有效地泛化（即解决POMDP）。这不直接适用于2p0s博弈，并且需要为每个agent训练N个策略 本文只需要为每个agent提供两个策略。 BACKGROUND AND NOTATION PPO image-20230128165027876 SAC an off-policy actor-critic algorithm, meaning it uses a buffer of past data to continually update its actor and critic functions image-20230128165346449 image-20230128165410762 image-20230128165310568 SAC updates a replay buffer D by collecting trajectories and performs gradient descent using ADAM on these objective functions NORMAL FORM GAMES a 2p0s game that can be specified by a matrix P ∈ R m×n. Each row in the matrix corresponds to an action of player 1 and each column corresponds to an action of player 2. The entry Pij specifies the payoff if player 1 plays i and player 2 plays j. image-20230128164554920 policy profile \\(\\pi=(\\pi^1,\\pi^2)\\)定义为\\(u^1(\\pi^1,\\pi^2)\\),2p0s:\\(u^1(\\pi^1,\\pi^2)=-u^2(\\pi^1,\\pi^2)\\) 定义best response \\(BR(\\pi^i)\\in \\arg \\max_{\\pi^{-i}}u(\\pi^{-i},\\pi^{i})\\) 定义exploitability .The exploitability of a policy \\(π^1\\) is defined to be how much worse the policy does against \\(BR(π^1 )\\) compared to how a Nash equilibrium strategy \\(π^1_∗\\) does against \\(BR(π^1_∗ )\\), image-20230128171006235 \\(\\epsilon-BR\\) a policy \\(π^2\\) is an ϵ-best response to \\(π^1\\) image-20230128171232225 FP choose a uniform policy\\(\\beta_u^1,\\beta_u^2\\) initialize their average policy to this uniform policy \\(\\pi^1=\\beta_u^1,\\pi^2=\\beta_u^2\\) agent computes a best response to the opponent’s average policy\\(\\beta^p_t=\\arg\\max_{\\beta^p}(\\beta^P,\\pi^{-p}_{t-1})\\) update their average policy\\(\\pi_t^p=\\frac{t-1}{t}\\pi^p_{t-1}+\\frac{1}{t}\\beta^p_t\\) 注意这里使用优化后的average policy\\(\\pi_t^p=\\frac{t-1}{t+1}\\pi^p_{t-1}+\\frac{2}{t+1}\\beta^p_t\\) image-20230122015608822 EXACT CASE 算法提出需要保证计算精确最佳响应 运行N次迭代（或直到收敛） 不是计算对手元策略的最佳响应，而是计算元策略和正则策略的线性组合的最佳响应。 假设进行到AdaptFP的第i次迭代，我们为每个玩家都有元策略\\(\\pi^1_i,\\pi^2_i\\) 将正则化策略初始化为空策略，并且在内部循环的每个迭代j中，我们计算 image-20230128182435540 其中 image-20230128182551501 从玩家1的角度来看（对玩家2来说是对称的），玩家2的内循环是对抗性地找到对对手的最佳响应，玩家1必须学习对adaptable的对手的最佳响应。（From player 1’s perspective (it is symmetric for player 2), the inner loop for player 2 is adversarially finding a best response to the opponent’s, and player 1 must learn a best response to an opponent that is allowed to adapt.） 在内部循环的末尾，将最新的最佳响应添加到元策略中。纯粹基于迭代来比较可利用性是不公平的，因为在自适应情况下，我们计算的最佳响应比在非自适应情况下多得多。因此，在的图中，我们显示了基于为每个玩家计算的最佳响应总数的结果。 证明AFP能够在2p0s问题上收敛至nash均衡 image-20230128183151531 image-20230122015523809 approximate case 近似的情况是类似的，但不使用一个内部循环寻找最佳响应，我们只是在近似player1的最佳对策的过程中不断训练player2的regularizer agent。在每次迭代\\(i\\)中，我们为玩家\\(p\\)训练一个策略\\(\\beta_i^p\\)，正则化因子（ regularizer）\\(\\pi_r^p\\)也是如此，该因子在每次迭代中会重新初始化。该策略使用强化学习算法（例如PPO或者SAC）进行训练，不受\\(\\beta_i^{-p}\\)约束, 在训练中，对手的行为是抽样前i个最佳对策\\(\\beta_i^p,...,\\beta_i^p\\)的平均值，也是如此正则化因子\\(\\pi_r^p\\)。 策略平均操作 DREAM (Steinberger et al., 2020) 在每episode开始时，我们根据权重从一组策略中采样一个策略，并使用该策略运行整个episode image-20230122024942411 Result image-20230128200128836 image-20230128200402443 image-20230128200306828 SAC的训练效率最高，观察包括agent当前位置、相对于球的位置、相对于球门的位置和对手位置的数据。我们通过将对手状态的观察滞后10个时间步，将不完美的信息引入游戏。 image-20230128200416845 image-20230128200445685 使用PPO强化学习算法，因为它在这种环境中最有效。观察结果包括关于agent当前位置以及对方agent的数据。为了在比赛中引入不完美的信息，对手的代理人落后10时间步长。在每episode开始时，我们根据给定的权重从集合中抽取一个策略，并在整个episode中使用该策略。我们在最佳响应的整个培训过程中重复该过程，这确保了策略的训练是与平均策略相比进行训练。 总结展望 在未来，仍有许多工作要开发鲁棒算法来解决连续控制博弈。这种类型的游戏非常重要，因为许多现实世界应用程序都涉及连续的动作空间，包括世界上涉及机器人的应用程序。虽然这项工作侧重于两人零和游戏，但我们认为这只是开发能够在真实世界交互中更广泛有效地发挥作用的代理的第一步。下一步是走向更复杂的连续状态和行动空间环境，包括同时涉及合作和竞争的环境。","categories":[],"tags":[{"name":"论文调研","slug":"论文调研","permalink":"https://klc1006.github.io/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/"},{"name":"FSP","slug":"FSP","permalink":"https://klc1006.github.io/tags/FSP/"}]},{"title":"【论文调研】Correlated Q-Learning笔记","slug":"Correlated Q-Learning","date":"2023-03-06T08:34:34.725Z","updated":"2023-03-06T09:17:02.996Z","comments":true,"path":"2023/03/06/Correlated Q-Learning/","link":"","permalink":"https://klc1006.github.io/2023/03/06/Correlated%20Q-Learning/","excerpt":"实验风向：设计多智能体Q学习算法来学习一般和马尔可夫博弈中的均衡策略 相关Qlearning：一种基于相关平衡解概念的算法 实验结果：相关Q学习优于纳什Q学习 相关Q在一般和（general sum）博弈中推广了纳什Q，因为相关均衡集包含纳什均衡集。相关Q也推广了 零和博弈，纳什均衡集和极小极大均衡集重合。","text":"实验风向：设计多智能体Q学习算法来学习一般和马尔可夫博弈中的均衡策略 相关Qlearning：一种基于相关平衡解概念的算法 实验结果：相关Q学习优于纳什Q学习 相关Q在一般和（general sum）博弈中推广了纳什Q，因为相关均衡集包含纳什均衡集。相关Q也推广了 零和博弈，纳什均衡集和极小极大均衡集重合。 一些便于理解相关均衡的例子： 交通信号。对于在交叉口相遇的两个agent，交通信号转换为概率为p的联合概率分布（stop，go）和概率为1−p的（go，stop）。没有为（go，go）或（stop，stop）分配概率质量。给出红色信号的agent的最佳行动是停止，而给出绿色信号的agent最佳行动是离开。 困难点：在一般和单次博弈中，存在具有多个值的多个均衡 解决：引入相关Q四种变量解决均衡选择问题 四种变量：功利、平等、财阀和独裁（utilitarian, egalitarian, plutocratic, and dictatorial） overview 本文组织如下。 一次性博弈中相关均衡的定义以及Marko博弈中相关平衡策略的定义。 在第3节中，我们定义了两种版本的多智能体Q-学习，一种是集中式的，另一种是分散式的，我们展示了相关-Q、Nash-Q和FF-Q是如何作为这些通用算法的特例出现的。 在第4节中，我们包括了零和和共同利益马尔可夫博弈的理论讨论，其中我们证明了相关Q学习的某些变体保证收敛到平稳均衡策略。 在第6节中，我们描述了将功利主义、平等主义、财阀主义和独裁相关的Q学习与Q学习、FF-Q和Nash-Q的两个变体进行比较的模拟实验 实验比较 使用各种多智能体Q学习算法进行的实验，其中包括三个网格游戏和网格足球，以及一组随机生成的游戏 我们四分之三的相关Q学习实现是集中的：功利的、平等的和财阀的。（ utilitarian, egalitarian, and plutocratic） 只有独裁（dictatorial）的变体是分散的：（每一个特工都像独裁者一样学习。） 网格游戏 image-20230226175032428 reward： 在GG1中，有两个不同的目标，每个目标值100分。在GG2中，有一个值100分的目标和两个障碍：如果一个代理人试图通过其中一个障碍，那么这一行动失败的概率为1/2。在GG3中，与GG2一样，有一个目标值100分，但没有随机过渡，奖励结构也不同：在开始时，如果两个代理都通过向上移动来避开中心状态，则每个代理将获得20分的奖励；此外，任何选择中心状态的代理都将获得25分的奖励（注：如果两个代理都选择中心状态，则会发生冲突，每个代理都会获得−25=25−50）。 平衡策略 探讨Q-learning学了什么： Q学习不会收敛，也不会学习均衡政策； 敌友Q学习趋同，但无需学习均衡政策； NE-Q的两个变体和CE-Q的所有四个变体学习均衡策略。 image-20230226175852687 测试阶段：反复进行网格游戏。显示了\\(10^4\\)次移动的平均得分。比赛的次数因agent的政策而异：有时agent直接向球门移动。 对于每个学习算法， 收敛性:表示Q值是否收敛； 平衡值:说明任何收敛Q值是否对应于均衡策略； 平衡游戏:说明测试期间的游戏轨迹是否符合平衡策略。 足球游戏 image-20230226181301477 圆圈代表球。如果玩家A移动W，他将失去球给球员B；但如果球员B移动E，试图偷球。 image-20230226183053723 基本Q-learning缺点：2p0s multiple Q-learners 没有达到收敛 在表4中，我们展示了这场足球比赛测试阶段的结果。 除Q学习者外，所有玩家都玩“好”游戏，这意味着每个玩家赢得的游戏数量大致相同；因此，分数接近0，0。 Friend-Q倾向于让对方快速获胜（观察比赛的数量），并且只因为格子足球的对称性，所以踢出了一场“好”的比赛。 总之，在网格足球这一双人零和马尔可夫博弈中，Q学习并不收敛。直观地说，这一结果的基本原理是明确的：Q学习寻求确定性最优策略，但在这个游戏中不存在这种策略。相关Q学习，如纳什Q学习，学习与foe-Q学习相同的Q值。然而，correlated-Q学习可能相关的均衡策略，而foe-Q和Nash-Q学习最小-最大均衡策略。","categories":[],"tags":[]},{"title":"【论文调研】DeepStack:Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker 笔记","slug":"DeepStack笔记","date":"2023-03-06T08:34:34.723Z","updated":"2023-03-06T09:11:10.263Z","comments":true,"path":"2023/03/06/DeepStack笔记/","link":"","permalink":"https://klc1006.github.io/2023/03/06/DeepStack%E7%AC%94%E8%AE%B0/","excerpt":"发表日期: 2017 机构: Alberta University 核心算法：公共树博弈建模+限制深度持续重新求解+深度反事实价值网络+CFR-D算法+范围（range）","text":"发表日期: 2017 机构: Alberta University 核心算法：公共树博弈建模+限制深度持续重新求解+深度反事实价值网络+CFR-D算法+范围（range） HUNL使用的公共树 游戏状态： 私人信息： 两张牌面朝下： 公共状态：包括面朝上放在桌子上的牌和下注顺序玩家的动作。（公共状态的可能序列形成公共树，每个公共状态都有一个关联的公共子树） 可利用性：预期效用与最佳反应对手之间的差异，以及纳什均衡下的预期效用。 匈牙利公共树的一部分。 节点表示公共状态，而边缘表示动作：红色和绿色表示玩家下注动作，绿色表示偶然出现的公共卡。 游戏在终端节点结束，显示为具有关联值的筹码。 对于没有玩家折叠的终端节点，其私人卡形成更强的扑克手的玩家接收状态值。 Deepstack 三个要素： 针对当前公共状态的完善的局部策略计算 用学习值函数避免推理到游戏结束的深度受限前瞻 一组受限的前瞻动作 image-20230217112502369 Continual re-solving 根据特定的解决策略采取了行动，但在某些公共状态下，忘记了这一策略。我们重新构建解决方案策略而不必再次求解整个游戏. 在比赛开始时，我们的范围是统一的，对手的反事实值被初始化为每一手私人牌的处理值。 当轮到我们采取行动时，我们使用存储的范围和对手值重新求解当前公共状态下的子树，并根据计算出的策略采取行动，在我们再次采取行动之前丢弃该策略。在每一个动作之后，无论是由玩家还是偶然发牌，我们都会更新我们的范围和对手的反事实值 根据以下规则： （i）自己的行动：将对手的反事实值替换为我们选择的行动的重新解决策略中计算的值。使用计算策略和贝叶斯规则更新我们自己的范围。 （ii）机会行动：将对手的反事实值替换为从上次重新求解中为该机会行动计算的值。更新我们自己的范围，将手放在给定新公共卡不可能的范围内。 （iii）对手行动：不需要改变我们的范围或对手的值。这些更新确保对手的反事实值满足我们的充分条件，并且整个过程产生纳什均衡的任意接近近似值 这些更新确保对手的反事实值满足我们的充分条件，并且整个过程产生纳什均衡的任意接近近似 该算法高效并避免对动作抽象方法所需的转换步骤的任何需要： 永远不会跟踪对手的范围，而只跟踪他们的反事实值。 不需要了解对手的行动来更新这些值（与传统的re-solvnig是一个重要区别） Limited depth lookahead via intuition. Input：就是对扑克游戏的描述：被发到个人手中的概率分布、游戏的赌注以及任何公开的牌 Output：对在这种游戏中持有某些牌的价值的估计。价值函数是一种直觉，是对 在任意扑克情境中发现自己的价值。在深度限制为四个动作的情况下，该方法将重新求解的游戏大小从开始时的\\(10^{160}\\)个决策点减少到比赛得分降至不超过\\(10^{17}\\)分。 Sound reasoning. DeepStack的深度有限的持续重新解决是合理的。如果DeepStack的直觉是“好的”，并且在每一个re-solving步骤中都使用了“足够的”计算，那么DeepStac将任意接近纳什均衡。 Relationship to heuristic search in perfect information games. 第一，如果不知道参与者如何以及为什么要达到公共状态，就无法对公共状态进行合理的重新解决。相反，必须保持两个额外的向量，即agent的范围和对手的反事实值，以用于重新求解。 第二，重新求解是一个迭代过程，它多次遍历前瞻树，而不是只遍历一次。每次迭代都需要再次查询评估函数，对于超出深度限制的每个公共状态，其范围都不同。 第三，达到深度极限时所需的评估函数在概念上比完美信息设置中更复杂。反实际值函数需要返回给定公共状态和玩家范围的值向量，而不是在游戏中给定单个状态时返回单个值。由于这种复杂性，为了学习这样的价值函数，我们使用了深度学习，这也成功地在完美的信息游戏中学习了复杂的评估函数 Relationship to abstraction-based approaches DeepStack限制其前瞻树中的动作数量，这与动作抽象非常相似。然而，DeepStack中的每一次重新解决都是从实际的公共状态开始的，因此它总是完全不了解当前的情况。该算法也从不需要使用对手的实际动作来获得正确的范围或对手的反事实值，从而避免对手下注的转换。 Overview image-20230217151735867 A. DeepStack在公共树中的推理总是为它在公共状态下可以持有的所有卡牌产生动作概率。它在游戏中保持着两个向量：它自己的范围（range）和它的对手的反事实值。随着游戏的进行，它自己的范围会在它执行完一个动作后通过贝叶斯规则使用它计算出的动作概率进行更新；对手的反事实值在“持续重新求解”中进行了更新。为了计算它必须行动时的动作概率，它使用它的范围和对手的反事实值来执行一个重新求解。为了使重新求解易于处理，它限制了玩家的可用动作，并且前瞻（lookahead）被限制在本回合结束。在重新求解过程中，使用DeepStack的已经学习的评估函数来近似公共状态的反事实值。 B. 评估函数用一个神经网络表示，该神经网络将公共状态和当前迭代的范围作为输入，输出两个玩家的反事实值（图3）。 C. 神经网络在玩游戏之前进行训练，通过生成随机的扑克情境（奖池大小、公共卡牌和玩家范围）并求解它们来生成训练例子。 Deep Counterfactual Value Networks image-20230217152502421 网络的输入是奖池大小、公共卡牌和玩家范围，它们首先被处理成手牌聚类。对7个全连接的隐藏层的输出进行后处理，以保证这些值满足零和约束，然后映射回一个反事实值的向量。 伪代码 DeepStack的限制深度持续重新求解算法的完整伪代码在算法S1中。从概念上讲，DeepStack算法可以分解为四个函数：RE-SOLVE, VALUES, UPDATE_SUBTREE_STRATEGIES和RANGE_GADGET。主要函数是RE-SOLVE，每次DeepStack需要执行动作时都会调用它。它迭代地调用其他函数来细化前瞻树的解。经过T轮迭代后，从要执行动作的子树的根结点处的近似均衡策略中采样一个动作。根据这一动作，DeepStack的范围\\(\\vec{v_1}\\)及其对手的反事实值 \\(\\vec{v_2}\\) 被更新，为下一个决策点做准备。 img img img","categories":[],"tags":[]},{"title":"【论文调研】Fictitious Self-Play in Extensive-Form Games 笔记","slug":"Fictitious Self-Play in Extensive-Form Games笔记","date":"2023-03-06T08:34:34.709Z","updated":"2023-03-06T09:17:33.053Z","comments":true,"path":"2023/03/06/Fictitious Self-Play in Extensive-Form Games笔记/","link":"","permalink":"https://klc1006.github.io/2023/03/06/Fictitious%20Self-Play%20in%20Extensive-Form%20Games%E7%AC%94%E8%AE%B0/","excerpt":"参考链接：https://zhuanlan.zhihu.com/p/38168216","text":"参考链接：https://zhuanlan.zhihu.com/p/38168216 Fictitious play（FP） 每个玩家在自我对局中保持两个策略：平均策略（average strategies）和最优反应策略（best response) ，每个玩家在对局中不断的采取对对手平均策略最优反应策略 best response：在对手策略固定的情形下，能够获得最大收益的策略 Extensive-Form Fictitious Play 一下公式展示这是如何通过加权组合其实现等效的行为策略来实现正常形式策略的混合 image-20230123021039239 Extensive-Form Fictitious Play： image-20230123021252113 image-20230123020931325 首先，它计算当前平均策略的最佳响应曲线。其次，它使用最佳响应曲线来更新平均策略曲线。第一个操作的假定要求在博弈状态的数量上是线性的。对于每个玩家，第二个操作可以独立于他们的对手执行，并且需要在玩家的信息状态的数量中进行线性工作。此外，如果使用确定性最佳响应，定理7的实现权重允许忽略玩家决策节点上除一个子树之外的所有子树。 Fictitious Self-Play FSP是一个机器学习框架，它以基于样本的方式和行为策略实现了广义的弱化虚拟游戏。XFP的维数很低。在每次迭代中，无论其相关性如何，都需要在游戏的所有状态下执行计算。然而，广义的弱化虚拟游戏只需要近似的最佳响应，甚至允许更新中出现一些扰动 FSP用机器学习算法代替了两种虚拟游戏操作，即最佳响应计算和平均策略更新。通过强化学习，从对手的平均策略中学习近似的最佳反应。平均策略更新可以被制定为一个有监督的学习任务，每个玩家学习自己行为的过渡模型。我们在第4.1节中介绍了基于强化学习的最佳响应计算，并在第4.2节中提出了基于监督学习的策略更新 RL 在这项工作中，我们使用FQI从抽样经验的数据集中学习。在每次迭代k时，FSP从自我游戏中抽取游戏片段。每个代理商都会将其体验添加到其重播内存\\(M_{RL}^{i}\\)中。数据以转换元组的集合\\((u_t,a_t,r_{t+1},u_{t+1})\\)的形式存储。每一集\\(\\eta=\\{(u_t,a_t,r_{t+1},u_{t+1})\\},0\\leq t \\leq T\\)，\\(T\\in\\mathbb{N}\\) 都包含有限数量的转换。我们使用固定尺寸的有限存储器。如果记忆已满，新剧集将以先发先出的顺序取代现有剧集。使用有限内存并逐步更新可能会偏离内存近似的基础分布。我们希望获得一个与对手的平均战略分布相似的记忆成分。这可以通过使用自适应策略配置文件来实现，该配置文件可以在代理的平均响应策略和最佳响应策略配置之间进行适当混合。 SL 我们将自己限制在简单模型中，这些模型计算在信息状态下采取行动的次数，或者累计各个策略采取每个行动的概率。这些模型可以在每次迭代k时使用来自\\(\\beta_k\\)的样本进行增量更新。模型更新需要一组采样元组\\((u_t^i,\\rho_t^i)\\)，其中\\((u_t^i)\\)是代理i的信息状态，而\\(\\rho_t^i\\)是agent在此经验采样时在此状态下追求的策略。对于每个元组\\((u_t,\\rho_t)\\)，更新会在信息状态下累积每个动作的权重 image-20230128205655159 Algorithm image-20230123022712765","categories":[],"tags":[]},{"title":"【论文调研】Hierarchical Abstraction, Distributed Equilibrium Computation, and Post-Processing, with Application to a Champion No-Limit Texas Hold'em Agent 笔记","slug":"hierarchical abstraction algorithm笔记","date":"2023-03-06T08:34:34.708Z","updated":"2023-03-06T09:18:01.716Z","comments":true,"path":"2023/03/06/hierarchical abstraction algorithm笔记/","link":"","permalink":"https://klc1006.github.io/2023/03/06/hierarchical%20abstraction%20algorithm%E7%AC%94%E8%AE%B0/","excerpt":"（看得有点迷糊，不确定理解得对不对）","text":"（看得有点迷糊，不确定理解得对不对） Main Abstraction Algorithm 让r是我们执行公共聚类的游戏的特殊回合 对于初始的\\(\\hat{r}\\)−1轮，我们使用任意算法\\(A_r\\)计算第r轮的（潜在不完美回忆）抽象。例如，在扑克中，最强agent在预循环中不使用抽象（即使他们确实使用了抽象，也不需要公共聚类，可以单独执行）。 将第\\(\\hat{r}\\)轮的公共状态聚集到C桶中。 image-20230217171729578 Algorithm for Computing Abstraction of Public Information(有点抽象，有点难懂) 一旦计算了该公共抽象，我们就为之前计算的每个公共桶分别计算私有信息的所有状态的从\\(\\hat{r}\\)到\\(R\\)的每一轮抽象,\\(A_r\\)，这些抽象可以使用任何任意的方法来计算。对于我们的扑克agent，我们使用了一种抽象算法，该算法之前已经被证明与\\(A_r\\)的算法一样性能良好 需要计算大量的公共翻牌 每一对公共状态设置距离函数（相似函数）\\(d_{i,j}\\) 使用聚类算法，和距离函数计算公共抽象 image-20230217172652417 Public Abstraction Clustering Algorithm output\\(c^T [i]\\) as the final abstraction image-20230217172804895 Equilibrium-Finding Algorithm 每当头部节点向集群叶发送数据时，我们的算法都会遇到叶间延迟，并且在接收响应时也会遇到这种延迟。每一次MCCFR迭代，这只相当于不到一毫秒。每次迭代大约需要15毫秒，因此延迟开销是负的。在这种开销很大的设置中，可以通过让子叶在每次迭代中获取更多样本，从而增加采样时间与延迟时间的比率，从而轻松地将其忽略不计。 由于头部节点只能在接收到来自所有集群叶的响应后才能继续，因此如果它们的MCCFR更新完成得比其他叶更快，则一些集群可能会空闲相当长的时间。尽管我们的抽象算法将博弈树均匀地划分为子叶，但这种情况还是会发生：在某些叶上，MCCFR计算的当前策略使得游戏路径更快地结束（例如，通过折叠扑克） 该算法开始于对优先级信息进行采样，并在头部叶片上执行MCCFR。当到达一个动作序列，该动作序列过渡到树的顶部（即，过渡到德克萨斯州Hold'em中的触发器）时，该算法将当前状态发送到K个子叶片C1、C2、…、…，每个子叶片CK然后从其公共桶（即，从分配给它的有效触发器Fk中的一个触发器）采样公共信息，并继续MCCFR的迭代。一旦所有子叶片完成其部分迭代，其计算值~uk将返回到头部叶片。头部叶片计算这些值的加权平均值，并根据公共信息的选择数量。 这确保了预期值是无偏的，也就是说，在预期中，每个失败都被平均加权。然后，头部节点继续其MCCFR的迭代，每当样本退出顶部时（遇到一个触发器序列），重复该过程，直到迭代完成。 在实践中（与伪代码中所示的不同），我们不是在每次采样超过树的顶部时与子节点通信（即，在Texas Hold'em中遇到一个触发器序列），而是使用两遍方法。在第一次通过时，我们只记录遇到了哪些连续（翻牌）序列。然后将这些序列发送给子叶片，以便它们可以计算这些序列的值；子叶片并行工作，但在每个子叶片中，分配给该叶片的连续序列被一个接一个地处理。然后，头部叶片执行与第一次相同的第二次传递，不同之处在于，每当样本超出树的顶部时（即，在Texas Hold'em中到达翻牌），都会使用从子叶片返回的值。 在每个子叶片中，即每个子集群中，我们实际上拥有并使用多个内核（为简单起见，pseu docode中未显示）。每当到达子集群时，每个核心都会得到相同的输入，但使用不同的随机数种子来选择集群中要处理的公共样本（德克萨斯州Hold'em的公共失败），以及如何根据MCCFR对其下的操作进行随机采样。考虑到游戏的性质，内核将以非常低的概率执行冗余工作，并且在集群的不同部分中迭代最多一次就会过时。（另一种选择是将树的一部分锁定在集群中，以防止核心在相同的信息集上工作，但这会带来开销，至少在Texas Hold'em似乎没有必要这样做。） image-20230217194944332 New Family of Post-Processing Technique（处理动作空间比较大的游戏） 后处理技术也被证明有助于缓解将平衡过度拟合到抽象中的问题，以及近似均衡发现可能最终导致不良行为的概率为正的问题。已经研究了两种方法，即阈值化和净化（Ganzfried、Sandholm和Waugh，2012年）。 在阈值化中，低于某个阈值的动作概率被设置为零，然后剩余的概率被重新规范化。净化是阈值化的特殊情况，其中具有最高概率的动作以概率1进行（随机均匀地打破关系）。 我们观察到，将反向映射和阈值化相结合会导致这样一个问题，即在动作空间的某个区域精细地离散动作不利于这些动作，因为平衡发现的概率质量在它们之间被稀释。为了缓解这个问题，我们建议将抽象动作划分为相似类，以便进行阈值处理（但不在阈值处理之后）。例如，在无限制扑克中，任何下注大小都可以达到玩家剩余筹码的数量。在给定的情况下，我们的投注抽象可以允许代理人折叠、调用、下注0.5罐、0.75罐、罐、1.5罐、2罐、5罐，等等。如果动作概率为（0.1，0.25，0.15，0.15，0.2，0.15,0,0），那么净化将选择召唤动作，而绝大多数质量（0.65）是在下注动作上。在这个例子中，我们下面详细介绍的方法将进行一个罐大小的下注（最高概率的下注动作）。 最后，我们观察到，偏向于减少方差的保守操作（例如扑克中的折叠动作）有助于强大的代理人（方差增加了较弱的对手获胜的概率）。我们的实验将表明，在TH中偏好保守的“折叠”动作也会增加预期值。一个原因可能是，如果一个代理人不确定在给定情况下应该做什么（平衡行动概率是混合的），那么该代理人很可能也会不确定，因此最好在这里结束游戏，而不是继续参与代理人较弱的游戏。 我们的新后处理技术结合了上面列出的所有想法。它首先将可用操作分为三类：折叠、调用和下注。如果折叠的概率超过阈值参数，我们将以概率1折叠。否则，我们会在折叠、调用和下注的“元动作”这三个选项之间进行净化。如果选择了下注，那么我们会在特定的下注动作中进行净化。 显然，这项技术有很多变化，因此它产生了一个系列，这取决于使用了什么阈值来确定使用保守动作（折叠）、如何将动作分组以进行阈值处理、在桶中使用了什么值以及在元动作（可能是多个）中使用了哪个阈值。","categories":[],"tags":[]},{"title":"【论文调研】The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems 笔记","slug":"the dynamics of reforcelearning","date":"2023-03-06T08:34:34.707Z","updated":"2023-03-06T09:09:05.958Z","comments":true,"path":"2023/03/06/the dynamics of reforcelearning/","link":"","permalink":"https://klc1006.github.io/2023/03/06/the%20dynamics%20of%20reforcelearning/","excerpt":"区分两个learner： 不知道（或忽略）其他agent的存在的 reforcelearner 明确尝试学习联合动作的价值及其对应者的策略的learner 以这两个视角：研究合作多智能体系统中的Q-learning 关注点： 部分动作可观测性，博弈结构，探索策略 对收敛到（最优和次优）纳什均衡和learned Q value的影响。","text":"区分两个learner： 不知道（或忽略）其他agent的存在的 reforcelearner 明确尝试学习联合动作的价值及其对应者的策略的learner 以这两个视角：研究合作多智能体系统中的Q-learning 关注点： 部分动作可观测性，博弈结构，探索策略 对收敛到（最优和次优）纳什均衡和learned Q value的影响。 Comparing Independent and Joint-Action Learners 简单协调游戏中的相对表现： ILs： 不会认为他们的任何一个选择（平均而言）比另一个更好。 A对于动作a0和a1的Q值将收敛到5，因为每当执行a0时，b0和b1执行的概率为0:5。（在任何时候，由于策略的随机性和学习率的衰减，我们都会预期学习到的Q值不会相同） 两个IL和JAL选择最佳联合动作的概率，作为它们之间交互次数的函数。 温度参数最初为T=16，在T+1次交互作用时衰减了\\(0.9^t\\)。 ILs的协调非常快 两个平衡点都没有偏好:两个平衡中的每一个都在大约一半的试验中获得。 JALs 固定数量的交互操作之后，JAL确实表现得更好。虽然JAL有更多的信息可供他们使用，但融合并没有显著增强。 虽然JAL能够区分不同联合行动的Q值，但他们使用这些信息的能力受到行动选择机制的限制。 “插入”勘探策略的单个行动的价值或多或少与ILs的Q值相同。区别在于：JAL使用显式分布和联合Q值来计算它们，而不是直接更新它们。 The Effects of Partial Action Observability image-20230305195054605 image-20230305200206449 agent i更新对所有的动作更新联合Q-value Pr is computed in the obvious way by i using its beliefs and Bayes rule. 观测模型越准确，agent的收敛速度就会更快 Convergence and Game Structure image-20230305201148923 该博弈具有三个确定性均衡 当k=-100时，agent A发现其第一和第三个动作是因为B的随机探索而没有吸引力 如果A是IL,a0和a2的平均reward会相当低 如果A是JAl，它对B的策略的信仰会使这些行动具有较低的预期价值。 k越接近零，agent发现其第一和第三行动不具吸引力的可能性越低。 image-20230305200819725 收敛到一个最优平衡点受到“惩罚”k的大小的影响。","categories":[],"tags":[]},{"title":"Lab 1","slug":"Lab1","date":"2022-09-08T06:37:48.725Z","updated":"2023-03-06T07:46:13.483Z","comments":true,"path":"2022/09/08/Lab1/","link":"","permalink":"https://klc1006.github.io/2022/09/08/Lab1/","excerpt":"实验题目 Accessing the Database The first laboratory exercise is to connect to a database, populate it with data, and run very simple SQL queries. In case a shared database is provided for course students, user accounts need to be created on the database. Otherwise, the lab should also cover setting up a database system. Information on setting up a database may be found here The next step is to connect to the database. Although most databases have their own text-based interface, we recommend using a graphical interface such as the database browser of the Netbeans IDE, or a database specific interface. More information on accessing these interfaces may be found here. The next step is to create tables and load sample data. Scripts for these tasks can be found here. Try out some queries, and see what they do. Some example queries: select * from instructor select name from instructor where dept_name = 'Comp. Sci.' and salary &gt; 70000 select * from instructor, department where instructor.dept_name = department.dept_name","text":"实验题目 Accessing the Database The first laboratory exercise is to connect to a database, populate it with data, and run very simple SQL queries. In case a shared database is provided for course students, user accounts need to be created on the database. Otherwise, the lab should also cover setting up a database system. Information on setting up a database may be found here The next step is to connect to the database. Although most databases have their own text-based interface, we recommend using a graphical interface such as the database browser of the Netbeans IDE, or a database specific interface. More information on accessing these interfaces may be found here. The next step is to create tables and load sample data. Scripts for these tasks can be found here. Try out some queries, and see what they do. Some example queries: select * from instructor select name from instructor where dept_name = 'Comp. Sci.' and salary &gt; 70000 select * from instructor, department where instructor.dept_name = department.dept_name 实验内容 step1 建立一个数据库系统 这里大家过程不再赘述 image-20220907205936723 step2 选择适合的IDE 这里使用vscode管理数据库，搭建结果如下： image-20220907205825794 step3 创建表 加载样本数据 image-20220907173559352 使用sql文件中的结构化查询语言建立表格，加载relation信息 ①创建数据库 1create database university; 检验是否创建成功 1show databases; ②创建表格 进入使用数据库university 1use university; 创建表格语言如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115create table classroom (building varchar(15), room_number varchar(7), capacity numeric(4,0), primary key (building, room_number) );create table department (dept_name varchar(20), building varchar(15), budget numeric(12,2) check (budget &gt; 0), primary key (dept_name) );create table course (course_id varchar(8), title varchar(50), dept_name varchar(20), credits numeric(2,0) check (credits &gt; 0), primary key (course_id), foreign key (dept_name) references department (dept_name) on delete set null );create table instructor (ID varchar(5), name varchar(20) not null, dept_name varchar(20), salary numeric(8,2) check (salary &gt; 29000), primary key (ID), foreign key (dept_name) references department (dept_name) on delete set null );create table section (course_id varchar(8), sec_id varchar(8), semester varchar(6) check (semester in (&#x27;Fall&#x27;, &#x27;Winter&#x27;, &#x27;Spring&#x27;, &#x27;Summer&#x27;)), year numeric(4,0) check (year &gt; 1701 and year &lt; 2100), building varchar(15), room_number varchar(7), time_slot_id varchar(4), primary key (course_id, sec_id, semester, year), foreign key (course_id) references course (course_id) on delete cascade, foreign key (building, room_number) references classroom (building, room_number) on delete set null );create table teaches (ID varchar(5), course_id varchar(8), sec_id varchar(8), semester varchar(6), year numeric(4,0), primary key (ID, course_id, sec_id, semester, year), foreign key (course_id, sec_id, semester, year) references section (course_id, sec_id, semester, year) on delete cascade, foreign key (ID) references instructor (ID) on delete cascade );create table student (ID varchar(5), name varchar(20) not null, dept_name varchar(20), tot_cred numeric(3,0) check (tot_cred &gt;= 0), primary key (ID), foreign key (dept_name) references department (dept_name) on delete set null );create table takes (ID varchar(5), course_id varchar(8), sec_id varchar(8), semester varchar(6), year numeric(4,0), grade varchar(2), primary key (ID, course_id, sec_id, semester, year), foreign key (course_id, sec_id, semester, year) references section (course_id, sec_id, semester, year) on delete cascade, foreign key (ID) references student (ID) on delete cascade );create table advisor (s_ID varchar(5), i_ID varchar(5), primary key (s_ID), foreign key (i_ID) references instructor (ID) on delete set null, foreign key (s_ID) references student (ID) on delete cascade );create table time_slot (time_slot_id varchar(4), day varchar(1), start_hr numeric(2) check (start_hr &gt;= 0 and start_hr &lt; 24), start_min numeric(2) check (start_min &gt;= 0 and start_min &lt; 60), end_hr numeric(2) check (end_hr &gt;= 0 and end_hr &lt; 24), end_min numeric(2) check (end_min &gt;= 0 and end_min &lt; 60), primary key (time_slot_id, day, start_hr, start_min) );create table prereq (course_id varchar(8), prereq_id varchar(8), primary key (course_id, prereq_id), foreign key (course_id) references course (course_id) on delete cascade, foreign key (prereq_id) references course (course_id) ); 输入加载文件语句： 1source D:\\db\\1\\DDL.sql 加载完成后，输入 1show tables; 查看表格信息如下 image-20220907200208225 ③加载样本信息 输入加载文件语句 1source D:\\db\\1\\smallRelationsInsertFile.sql 内容： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150delete from prereq;delete from time_slot;delete from advisor;delete from takes;delete from student;delete from teaches;delete from section;delete from instructor;delete from course;delete from department;delete from classroom;insert into classroom values (&#x27;Packard&#x27;, &#x27;101&#x27;, &#x27;500&#x27;);insert into classroom values (&#x27;Painter&#x27;, &#x27;514&#x27;, &#x27;10&#x27;);insert into classroom values (&#x27;Taylor&#x27;, &#x27;3128&#x27;, &#x27;70&#x27;);insert into classroom values (&#x27;Watson&#x27;, &#x27;100&#x27;, &#x27;30&#x27;);insert into classroom values (&#x27;Watson&#x27;, &#x27;120&#x27;, &#x27;50&#x27;);insert into department values (&#x27;Biology&#x27;, &#x27;Watson&#x27;, &#x27;90000&#x27;);insert into department values (&#x27;Comp. Sci.&#x27;, &#x27;Taylor&#x27;, &#x27;100000&#x27;);insert into department values (&#x27;Elec. Eng.&#x27;, &#x27;Taylor&#x27;, &#x27;85000&#x27;);insert into department values (&#x27;Finance&#x27;, &#x27;Painter&#x27;, &#x27;120000&#x27;);insert into department values (&#x27;History&#x27;, &#x27;Painter&#x27;, &#x27;50000&#x27;);insert into department values (&#x27;Music&#x27;, &#x27;Packard&#x27;, &#x27;80000&#x27;);insert into department values (&#x27;Physics&#x27;, &#x27;Watson&#x27;, &#x27;70000&#x27;);insert into course values (&#x27;BIO-101&#x27;, &#x27;Intro. to Biology&#x27;, &#x27;Biology&#x27;, &#x27;4&#x27;);insert into course values (&#x27;BIO-301&#x27;, &#x27;Genetics&#x27;, &#x27;Biology&#x27;, &#x27;4&#x27;);insert into course values (&#x27;BIO-399&#x27;, &#x27;Computational Biology&#x27;, &#x27;Biology&#x27;, &#x27;3&#x27;);insert into course values (&#x27;CS-101&#x27;, &#x27;Intro. to Computer Science&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;4&#x27;);insert into course values (&#x27;CS-190&#x27;, &#x27;Game Design&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;4&#x27;);insert into course values (&#x27;CS-315&#x27;, &#x27;Robotics&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;3&#x27;);insert into course values (&#x27;CS-319&#x27;, &#x27;Image Processing&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;3&#x27;);insert into course values (&#x27;CS-347&#x27;, &#x27;Database System Concepts&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;3&#x27;);insert into course values (&#x27;EE-181&#x27;, &#x27;Intro. to Digital Systems&#x27;, &#x27;Elec. Eng.&#x27;, &#x27;3&#x27;);insert into course values (&#x27;FIN-201&#x27;, &#x27;Investment Banking&#x27;, &#x27;Finance&#x27;, &#x27;3&#x27;);insert into course values (&#x27;HIS-351&#x27;, &#x27;World History&#x27;, &#x27;History&#x27;, &#x27;3&#x27;);insert into course values (&#x27;MU-199&#x27;, &#x27;Music Video Production&#x27;, &#x27;Music&#x27;, &#x27;3&#x27;);insert into course values (&#x27;PHY-101&#x27;, &#x27;Physical Principles&#x27;, &#x27;Physics&#x27;, &#x27;4&#x27;);insert into instructor values (&#x27;10101&#x27;, &#x27;Srinivasan&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;65000&#x27;);insert into instructor values (&#x27;12121&#x27;, &#x27;Wu&#x27;, &#x27;Finance&#x27;, &#x27;90000&#x27;);insert into instructor values (&#x27;15151&#x27;, &#x27;Mozart&#x27;, &#x27;Music&#x27;, &#x27;40000&#x27;);insert into instructor values (&#x27;22222&#x27;, &#x27;Einstein&#x27;, &#x27;Physics&#x27;, &#x27;95000&#x27;);insert into instructor values (&#x27;32343&#x27;, &#x27;El Said&#x27;, &#x27;History&#x27;, &#x27;60000&#x27;);insert into instructor values (&#x27;33456&#x27;, &#x27;Gold&#x27;, &#x27;Physics&#x27;, &#x27;87000&#x27;);insert into instructor values (&#x27;45565&#x27;, &#x27;Katz&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;75000&#x27;);insert into instructor values (&#x27;58583&#x27;, &#x27;Califieri&#x27;, &#x27;History&#x27;, &#x27;62000&#x27;);insert into instructor values (&#x27;76543&#x27;, &#x27;Singh&#x27;, &#x27;Finance&#x27;, &#x27;80000&#x27;);insert into instructor values (&#x27;76766&#x27;, &#x27;Crick&#x27;, &#x27;Biology&#x27;, &#x27;72000&#x27;);insert into instructor values (&#x27;83821&#x27;, &#x27;Brandt&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;92000&#x27;);insert into instructor values (&#x27;98345&#x27;, &#x27;Kim&#x27;, &#x27;Elec. Eng.&#x27;, &#x27;80000&#x27;);insert into section values (&#x27;BIO-101&#x27;, &#x27;1&#x27;, &#x27;Summer&#x27;, &#x27;2017&#x27;, &#x27;Painter&#x27;, &#x27;514&#x27;, &#x27;B&#x27;);insert into section values (&#x27;BIO-301&#x27;, &#x27;1&#x27;, &#x27;Summer&#x27;, &#x27;2018&#x27;, &#x27;Painter&#x27;, &#x27;514&#x27;, &#x27;A&#x27;);insert into section values (&#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;Packard&#x27;, &#x27;101&#x27;, &#x27;H&#x27;);insert into section values (&#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Packard&#x27;, &#x27;101&#x27;, &#x27;F&#x27;);insert into section values (&#x27;CS-190&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;, &#x27;Taylor&#x27;, &#x27;3128&#x27;, &#x27;E&#x27;);insert into section values (&#x27;CS-190&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;, &#x27;Taylor&#x27;, &#x27;3128&#x27;, &#x27;A&#x27;);insert into section values (&#x27;CS-315&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Watson&#x27;, &#x27;120&#x27;, &#x27;D&#x27;);insert into section values (&#x27;CS-319&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Watson&#x27;, &#x27;100&#x27;, &#x27;B&#x27;);insert into section values (&#x27;CS-319&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Taylor&#x27;, &#x27;3128&#x27;, &#x27;C&#x27;);insert into section values (&#x27;CS-347&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;Taylor&#x27;, &#x27;3128&#x27;, &#x27;A&#x27;);insert into section values (&#x27;EE-181&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;, &#x27;Taylor&#x27;, &#x27;3128&#x27;, &#x27;C&#x27;);insert into section values (&#x27;FIN-201&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Packard&#x27;, &#x27;101&#x27;, &#x27;B&#x27;);insert into section values (&#x27;HIS-351&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Painter&#x27;, &#x27;514&#x27;, &#x27;C&#x27;);insert into section values (&#x27;MU-199&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;Packard&#x27;, &#x27;101&#x27;, &#x27;D&#x27;);insert into section values (&#x27;PHY-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;Watson&#x27;, &#x27;100&#x27;, &#x27;A&#x27;);insert into teaches values (&#x27;10101&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;);insert into teaches values (&#x27;10101&#x27;, &#x27;CS-315&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;10101&#x27;, &#x27;CS-347&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;);insert into teaches values (&#x27;12121&#x27;, &#x27;FIN-201&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;15151&#x27;, &#x27;MU-199&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;22222&#x27;, &#x27;PHY-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;);insert into teaches values (&#x27;32343&#x27;, &#x27;HIS-351&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;45565&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;45565&#x27;, &#x27;CS-319&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;76766&#x27;, &#x27;BIO-101&#x27;, &#x27;1&#x27;, &#x27;Summer&#x27;, &#x27;2017&#x27;);insert into teaches values (&#x27;76766&#x27;, &#x27;BIO-301&#x27;, &#x27;1&#x27;, &#x27;Summer&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;83821&#x27;, &#x27;CS-190&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;);insert into teaches values (&#x27;83821&#x27;, &#x27;CS-190&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;);insert into teaches values (&#x27;83821&#x27;, &#x27;CS-319&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;);insert into teaches values (&#x27;98345&#x27;, &#x27;EE-181&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;);insert into student values (&#x27;00128&#x27;, &#x27;Zhang&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;102&#x27;);insert into student values (&#x27;12345&#x27;, &#x27;Shankar&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;32&#x27;);insert into student values (&#x27;19991&#x27;, &#x27;Brandt&#x27;, &#x27;History&#x27;, &#x27;80&#x27;);insert into student values (&#x27;23121&#x27;, &#x27;Chavez&#x27;, &#x27;Finance&#x27;, &#x27;110&#x27;);insert into student values (&#x27;44553&#x27;, &#x27;Peltier&#x27;, &#x27;Physics&#x27;, &#x27;56&#x27;);insert into student values (&#x27;45678&#x27;, &#x27;Levy&#x27;, &#x27;Physics&#x27;, &#x27;46&#x27;);insert into student values (&#x27;54321&#x27;, &#x27;Williams&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;54&#x27;);insert into student values (&#x27;55739&#x27;, &#x27;Sanchez&#x27;, &#x27;Music&#x27;, &#x27;38&#x27;);insert into student values (&#x27;70557&#x27;, &#x27;Snow&#x27;, &#x27;Physics&#x27;, &#x27;0&#x27;);insert into student values (&#x27;76543&#x27;, &#x27;Brown&#x27;, &#x27;Comp. Sci.&#x27;, &#x27;58&#x27;);insert into student values (&#x27;76653&#x27;, &#x27;Aoi&#x27;, &#x27;Elec. Eng.&#x27;, &#x27;60&#x27;);insert into student values (&#x27;98765&#x27;, &#x27;Bourikas&#x27;, &#x27;Elec. Eng.&#x27;, &#x27;98&#x27;);insert into student values (&#x27;98988&#x27;, &#x27;Tanaka&#x27;, &#x27;Biology&#x27;, &#x27;120&#x27;);insert into takes values (&#x27;00128&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;00128&#x27;, &#x27;CS-347&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;A-&#x27;);insert into takes values (&#x27;12345&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;C&#x27;);insert into takes values (&#x27;12345&#x27;, &#x27;CS-190&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;12345&#x27;, &#x27;CS-315&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;12345&#x27;, &#x27;CS-347&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;19991&#x27;, &#x27;HIS-351&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;B&#x27;);insert into takes values (&#x27;23121&#x27;, &#x27;FIN-201&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;C+&#x27;);insert into takes values (&#x27;44553&#x27;, &#x27;PHY-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;B-&#x27;);insert into takes values (&#x27;45678&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;F&#x27;);insert into takes values (&#x27;45678&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;B+&#x27;);insert into takes values (&#x27;45678&#x27;, &#x27;CS-319&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;B&#x27;);insert into takes values (&#x27;54321&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;A-&#x27;);insert into takes values (&#x27;54321&#x27;, &#x27;CS-190&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;, &#x27;B+&#x27;);insert into takes values (&#x27;55739&#x27;, &#x27;MU-199&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;A-&#x27;);insert into takes values (&#x27;76543&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;76543&#x27;, &#x27;CS-319&#x27;, &#x27;2&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;76653&#x27;, &#x27;EE-181&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2017&#x27;, &#x27;C&#x27;);insert into takes values (&#x27;98765&#x27;, &#x27;CS-101&#x27;, &#x27;1&#x27;, &#x27;Fall&#x27;, &#x27;2017&#x27;, &#x27;C-&#x27;);insert into takes values (&#x27;98765&#x27;, &#x27;CS-315&#x27;, &#x27;1&#x27;, &#x27;Spring&#x27;, &#x27;2018&#x27;, &#x27;B&#x27;);insert into takes values (&#x27;98988&#x27;, &#x27;BIO-101&#x27;, &#x27;1&#x27;, &#x27;Summer&#x27;, &#x27;2017&#x27;, &#x27;A&#x27;);insert into takes values (&#x27;98988&#x27;, &#x27;BIO-301&#x27;, &#x27;1&#x27;, &#x27;Summer&#x27;, &#x27;2018&#x27;, null);insert into advisor values (&#x27;00128&#x27;, &#x27;45565&#x27;);insert into advisor values (&#x27;12345&#x27;, &#x27;10101&#x27;);insert into advisor values (&#x27;23121&#x27;, &#x27;76543&#x27;);insert into advisor values (&#x27;44553&#x27;, &#x27;22222&#x27;);insert into advisor values (&#x27;45678&#x27;, &#x27;22222&#x27;);insert into advisor values (&#x27;76543&#x27;, &#x27;45565&#x27;);insert into advisor values (&#x27;76653&#x27;, &#x27;98345&#x27;);insert into advisor values (&#x27;98765&#x27;, &#x27;98345&#x27;);insert into advisor values (&#x27;98988&#x27;, &#x27;76766&#x27;);insert into time_slot values (&#x27;A&#x27;, &#x27;M&#x27;, &#x27;8&#x27;, &#x27;0&#x27;, &#x27;8&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;A&#x27;, &#x27;W&#x27;, &#x27;8&#x27;, &#x27;0&#x27;, &#x27;8&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;A&#x27;, &#x27;F&#x27;, &#x27;8&#x27;, &#x27;0&#x27;, &#x27;8&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;B&#x27;, &#x27;M&#x27;, &#x27;9&#x27;, &#x27;0&#x27;, &#x27;9&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;B&#x27;, &#x27;W&#x27;, &#x27;9&#x27;, &#x27;0&#x27;, &#x27;9&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;B&#x27;, &#x27;F&#x27;, &#x27;9&#x27;, &#x27;0&#x27;, &#x27;9&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;C&#x27;, &#x27;M&#x27;, &#x27;11&#x27;, &#x27;0&#x27;, &#x27;11&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;C&#x27;, &#x27;W&#x27;, &#x27;11&#x27;, &#x27;0&#x27;, &#x27;11&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;C&#x27;, &#x27;F&#x27;, &#x27;11&#x27;, &#x27;0&#x27;, &#x27;11&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;D&#x27;, &#x27;M&#x27;, &#x27;13&#x27;, &#x27;0&#x27;, &#x27;13&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;D&#x27;, &#x27;W&#x27;, &#x27;13&#x27;, &#x27;0&#x27;, &#x27;13&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;D&#x27;, &#x27;F&#x27;, &#x27;13&#x27;, &#x27;0&#x27;, &#x27;13&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;E&#x27;, &#x27;T&#x27;, &#x27;10&#x27;, &#x27;30&#x27;, &#x27;11&#x27;, &#x27;45 &#x27;);insert into time_slot values (&#x27;E&#x27;, &#x27;R&#x27;, &#x27;10&#x27;, &#x27;30&#x27;, &#x27;11&#x27;, &#x27;45 &#x27;);insert into time_slot values (&#x27;F&#x27;, &#x27;T&#x27;, &#x27;14&#x27;, &#x27;30&#x27;, &#x27;15&#x27;, &#x27;45 &#x27;);insert into time_slot values (&#x27;F&#x27;, &#x27;R&#x27;, &#x27;14&#x27;, &#x27;30&#x27;, &#x27;15&#x27;, &#x27;45 &#x27;);insert into time_slot values (&#x27;G&#x27;, &#x27;M&#x27;, &#x27;16&#x27;, &#x27;0&#x27;, &#x27;16&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;G&#x27;, &#x27;W&#x27;, &#x27;16&#x27;, &#x27;0&#x27;, &#x27;16&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;G&#x27;, &#x27;F&#x27;, &#x27;16&#x27;, &#x27;0&#x27;, &#x27;16&#x27;, &#x27;50&#x27;);insert into time_slot values (&#x27;H&#x27;, &#x27;W&#x27;, &#x27;10&#x27;, &#x27;0&#x27;, &#x27;12&#x27;, &#x27;30&#x27;);insert into prereq values (&#x27;BIO-301&#x27;, &#x27;BIO-101&#x27;);insert into prereq values (&#x27;BIO-399&#x27;, &#x27;BIO-101&#x27;);insert into prereq values (&#x27;CS-190&#x27;, &#x27;CS-101&#x27;);insert into prereq values (&#x27;CS-315&#x27;, &#x27;CS-101&#x27;);insert into prereq values (&#x27;CS-319&#x27;, &#x27;CS-101&#x27;);insert into prereq values (&#x27;CS-347&#x27;, &#x27;CS-101&#x27;);insert into prereq values (&#x27;EE-181&#x27;, &#x27;PHY-101&#x27;); 输入加载文件语句 1source D:\\db\\1\\largeRelationsInsertFile.sql 内容： 123456789101112131415161718192021delete from prereq;delete from time_slot;delete from advisor;delete from takes;delete from student;delete from teaches;delete from section;delete from instructor;delete from course;delete from department;delete from classroom;insert into time_slot values ( &#x27;A&#x27;, &#x27;M&#x27;, 8, 0, 8, 50);insert into time_slot values ( &#x27;A&#x27;, &#x27;W&#x27;, 8, 0, 8, 50);insert into time_slot values ( &#x27;A&#x27;, &#x27;F&#x27;, 8, 0, 8, 50);insert into time_slot values ( &#x27;B&#x27;, &#x27;M&#x27;, 9, 0, 9, 50);insert into time_slot values ( &#x27;B&#x27;, &#x27;W&#x27;, 9, 0, 9, 50);insert into time_slot values ( &#x27;B&#x27;, &#x27;F&#x27;, 9, 0, 9, 50);insert into time_slot values ( &#x27;C&#x27;, &#x27;M&#x27;, 11, 0, 11, 50);insert into time_slot values ( &#x27;C&#x27;, &#x27;W&#x27;, 11, 0, 11, 50);insert into time_slot values ( &#x27;C&#x27;, &#x27;F&#x27;, 11, 0, 11, 50);... 示例结果如下： 输入 1show columns from classroom; 1show columns from instructor; image-20220907202620296 如图显示了两个表格的含有的信息，包括属性，数据类型等等。 step4 尝试一些sql查询语句 ```sql select * from instructor 123456789 **结果输出：** ![image-20220907204404000](https://s2.loli.net/2022/09/08/Uo48IqZ9sf1tFXv.png) ![image-20220907204412921](https://s2.loli.net/2022/09/08/XiRmdVZlLz1MyfN.png)* ```sql select name from instructor where dept_name = &#x27;Comp. Sci.&#x27; and salary &gt; 70000 结果输出： image-20220907204459661 sql select * from instructor, department where instructor.dept_name = department.dept_name 结果输出： image-20220907204550046 image-20220907204600120 总结 本实验，简单完成数据库的创建，创建列表，以及信息加载，学会使用sql结构化查询语句。 \\[ \\begin{equation} \\left\\{ \\begin{array}{lr} x=\\dfrac{3\\pi}{2}(1+2t)\\cos(\\dfrac{3\\pi}{2}(1+2t)), &amp; \\\\ y=s, &amp; 0 \\leq s \\leq L,|t| \\leq1. \\\\ z=\\dfrac{3\\pi}{2}(1+2t)\\sin(\\dfrac{3\\pi}{2}(1+2t)), &amp; \\end{array} \\right. \\end{equation} \\]","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-08-25T08:46:02.561Z","updated":"2022-08-25T08:46:02.562Z","comments":true,"path":"2022/08/25/hello-world/","link":"","permalink":"https://klc1006.github.io/2022/08/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"论文调研","slug":"论文调研","permalink":"https://klc1006.github.io/tags/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/"},{"name":"FSP","slug":"FSP","permalink":"https://klc1006.github.io/tags/FSP/"}]}